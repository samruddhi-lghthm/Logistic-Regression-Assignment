{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical Question - Answers"
      ],
      "metadata": {
        "id": "-5_yY10Xtf9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression ?"
      ],
      "metadata": {
        "id": "f-Yor3dGtVlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: Logistic Regression is a statistical method used to predict the probability of a categorical outcome (like \"yes\" or \"no\") based on one or more independent variables, while Linear Regression predicts a continuous outcome value using a linear relationship between variables; essentially, Logistic Regression is used for classification problems, whereas Linear Regression is used for regression problems where the dependent variable is continuous."
      ],
      "metadata": {
        "id": "stlPA0LQtvAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression ?\n"
      ],
      "metadata": {
        "id": "ZfNzL2xdt-Z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression and its function. (5.2) S ( x ) = 1 2 [ 1 + tanh ⁡ x 2 ] , tanh ⁡ x = e x − x − x e x + e − x . It is easy to see that S → + 1 as x → + ∞ , whereas S → 0 as x → − ∞ . Thus the range of S is ( 0 , 1 )"
      ],
      "metadata": {
        "id": "guH8QP5yEEjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Why do we use the Sigmoid function in Logistic Regression?"
      ],
      "metadata": {
        "id": "NEh7fEhcF73I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Sigmoid function is used in Logistic Regression because it effectively maps any real-valued input to a value between 0 and 1, which perfectly represents the probability of a binary outcome, making it ideal for predicting probabilities in a binary classification problem where the goal is to determine if an event will occur or not."
      ],
      "metadata": {
        "id": "dqAPNDJcF8Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression?"
      ],
      "metadata": {
        "id": "qw2M5oGQF8Ns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost function for logistic regression is the average of the log loss over all training examples. It is often referred to as the cross-entropy cost function and is designed to optimize the parameters to minimize the prediction error for binary classification tasks."
      ],
      "metadata": {
        "id": "1-1QVpXbF8Rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed?"
      ],
      "metadata": {
        "id": "_GG2Um6BF8Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Logistic Regression, \"Regularization\" is a technique used to prevent overfitting by adding a penalty term to the cost function, effectively forcing the model to learn less complex relationships between features and the target variable, thus improving its ability to generalize to new data by avoiding overly complex fits to the training data alone; essentially, it helps the model perform better on unseen data by limiting the magnitude of the model coefficients."
      ],
      "metadata": {
        "id": "bGA37qnGF8Y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6.Explain the difference between Lasso, Ridge, and Elastic Net regression."
      ],
      "metadata": {
        "id": "_ddRy_X7IOWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso completely removes unnecessary features by setting their coefficients to zero. Ridge makes all coefficients smaller but doesn't set them to zero. Elastic Net removes some features and reduces others, balancing both. It is good when all features are useful, but you want to reduce their impact."
      ],
      "metadata": {
        "id": "4BrrizFqIOjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge?"
      ],
      "metadata": {
        "id": "iOiQXQTcIOmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Elastic Net instead of Lasso or Ridge when you have a dataset with highly correlated features, as it combines the strengths of both by performing feature selection while also handling multicollinearity better than Lasso alone, making it ideal for situations where you need to select relevant variables while also dealing with correlated predictors in your data."
      ],
      "metadata": {
        "id": "M1lo_LlbIOpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?"
      ],
      "metadata": {
        "id": "cCQQ61UdIOr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Logistic Regression, the regularization parameter (λ) controls the strength of the penalty applied to the model coefficients, essentially dictating the trade-off between fitting the training data closely and preventing overfitting by keeping the coefficients smaller; a higher λ value leads to stronger regularization, resulting in a simpler model with potentially better generalization ability on new data, while a lower λ allows the model to fit the training data more precisely but may lead to overfitting."
      ],
      "metadata": {
        "id": "3wLoi00_IOvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?"
      ],
      "metadata": {
        "id": "ko5DsAfxIOyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Independence of observations: The data points should not be from any dependent samples design, such as before-after measurements or matched pairings\n",
        "* Linearity: The independent variables are linearly related to the log odds.   \n",
        "\n",
        "*   Absence of multicollinearity: Two or more independent variables in the model are not approximately determined by a linear combination of other independent variables.\n",
        "*   Lack of outliers: There are no strongly influential outliers.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RpZFtV9tIO1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks?"
      ],
      "metadata": {
        "id": "AtjPulTtIO9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Alternatives to Logistic Regression for classification tasks include:\n",
        "\n",
        "* Decision Trees: Easy to interpret and implement, good for non-linear\n",
        "relationships between features and target variable.\n",
        "\n",
        "*  Support Vector Machines (SVMs): Effective for high-dimensional data and complex decision boundaries.\n",
        "\n",
        "*   Neural Networks: Powerful for complex patterns, especially when combined with deep learning architectures.\n",
        "\n",
        "*   Random Forests: Ensemble of decision trees, often providing better generalization performance compared to single decision trees.\n",
        "\n",
        "*   Naive Bayes: Simple and efficient for large datasets, particularly when features are conditionally independent.\n",
        "\n",
        "*   K-Nearest Neighbors (KNN): Classifies data points based on the majority class of their nearest neighbors\n",
        "\n",
        "*   Boosting Algorithms (e.g., XGBoost): Combines multiple weak learners to create a strong predictor.\n",
        "\n",
        "*   Probit Regression: Similar to Logistic Regression but uses the probit function instead of the logit function.\n",
        "\n",
        "*   Poisson Regression: Suitable when the target variable represents count data.\n",
        "\n",
        "*   Generalized Linear Models (GLMs): Can handle different types of\n",
        "distributions beyond the binomial distribution used in Logistic Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "jqhH_YLHIPBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?"
      ],
      "metadata": {
        "id": "IRNz6xxVQCU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Metrics like accuracy, precision, recall are good ways to evaluate classification models for balanced datasets, but if the data is imbalanced then other methods like ROC/AUC perform better in evaluating the model performance."
      ],
      "metadata": {
        "id": "M2c6OAM3QCmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  How does class imbalance affect Logistic Regression?"
      ],
      "metadata": {
        "id": "6eLnpGSDQCpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class imbalance significantly affects Logistic Regression by causing the model to become biased towards the majority class, leading to poor predictive performance on the minority class, especially when dealing with rare events, as the model prioritizes correctly classifying the more numerous data points, often neglecting the important information within the minority class."
      ],
      "metadata": {
        "id": "-TnJThxGQCsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression?"
      ],
      "metadata": {
        "id": "uqZdJuCgUM6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning involves selecting the optimal values of hyperparameters, which affect the performance of the Logistic Regression model. Some common hyperparameters that can be tuned include the learning rate, regularization strength, batch size, and number of iterations."
      ],
      "metadata": {
        "id": "lY9lw8bcUM2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?"
      ],
      "metadata": {
        "id": "-RYYqm0DUMiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones; For multiclass problems, all solvers except 'liblinear' minimize the full multinomial loss; 'liblinear' can only handle binary classification by default."
      ],
      "metadata": {
        "id": "-yYgVjvIUMem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How is Logistic Regression extended for multiclass classification?"
      ],
      "metadata": {
        "id": "rb_kn0RdWwq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multinomial Logistic Regression (Softmax Regression):\n",
        "Unlike the One-vs-Rest approach, Multinomial Logistic Regression directly extends the binary Logistic Regression to handle multiple classes. It uses the softmax activation function to calculate probabilities for each class."
      ],
      "metadata": {
        "id": "CIDRCxZ-WwYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What are the advantages and disadvantages of Logistic Regression?"
      ],
      "metadata": {
        "id": "aUIsvnKbWwU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression offers the following advantages: Easy to set up. The main advantage of logistic regression is that it is a simple model which is much easier to set up and train than other machine learning models such as neural networks and AI applications. Efficient algorithms."
      ],
      "metadata": {
        "id": "Zh4tWfEkXmin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression?"
      ],
      "metadata": {
        "id": "wLnZtHb9Xy70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is commonly used for classification tasks, including predicting whether an email is spam, identifying fraudulent credit card transactions, diagnosing medical conditions, assessing customer churn in marketing, and detecting financial anomalies like fraud, all thanks to its ability to predict the probability of a binary outcome based on input features."
      ],
      "metadata": {
        "id": "ovi7kWx6X8qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?"
      ],
      "metadata": {
        "id": "_zynmsWPYPnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)∈{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits."
      ],
      "metadata": {
        "id": "UIOi2TW8Ym_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?"
      ],
      "metadata": {
        "id": "edQPdrddYzTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When choosing between One-vs-Rest (OvR) and Softmax for multiclass classification, prefer Softmax if you need a more nuanced probability distribution across all classes, especially when dealing with well-balanced class distributions, while OvR is better suited for situations where you have a large number of classes and computational efficiency is a primary concern; OvR creates separate binary classifiers for each class, which can be less accurate when classes are not well-separated, while Softmax considers all classes simultaneously, leading to potentially better class probability estimations."
      ],
      "metadata": {
        "id": "HuP7sqavY8ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?"
      ],
      "metadata": {
        "id": "wv66yOo5ZF90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In logistic regression, coefficients represent the change in the \"log odds\" of the outcome variable for a one-unit increase in the corresponding predictor variable, holding all other variables constant; essentially, they tell you how much the log of the odds of the event occurring changes when a predictor variable increases by one unit, which is usually interpreted by exponentiating the coefficient to get the odds ratio, signifying the multiplicative effect on the odds of the event happening."
      ],
      "metadata": {
        "id": "xST7HzSyZPgO"
      }
    }
  ]
}